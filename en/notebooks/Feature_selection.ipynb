{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature selection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR_4UtziOJrL",
        "colab_type": "text"
      },
      "source": [
        "# Feature engineering: selection\n",
        "\n",
        "As we have previously discussed, the first step in a machine learning pipeline is data preparation, and in previous notebooks we have discussed outliers, imputation, encoding, and transformation. An eager approach would be to feed right away the prepared data to estimators. In some situations, that is possible and could be done. Most often, however, providing estimators raw features is not advisable, either because the number of features is too large (e.g. due to one-hot encoding) or the information they convey is redundant. \n",
        "\n",
        "**Feature engineering** is the ML pipeline step where we select the most relevant features, or even produce new features that are better representative of the information conveyed by the original ones. Two sets of approaches can be adopted:\n",
        "- **Selection:** a subset of the original features is selected.\n",
        "- **Extraction:** new features are produced from the original ones.\n",
        "\n",
        "In this notebook, we'll discuss **feature selection** approaches, particularly the ones provided by scikit-learn. In a nutshell, these methods can be categorized as *filter*, *wrapper*, or *embedded*. To understand these from a practical perspective, let's load the Boston house price prediction dataset provided by scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMJWa9XCdBvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "boston_data = load_boston()\n",
        "print(boston_data.DESCR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2L_Ut5RdZVp",
        "colab_type": "text"
      },
      "source": [
        "The data is provided as a structure where the `data` and `feature_names` attributes respectively represent the input characteristics and their names, and the `target` attribute represents the characteristic to be predicted (median market value):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFWkp4PQdYmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pd.DataFrame(boston_data.data, columns = boston_data.feature_names)\n",
        "y = pd.Series(boston_data.target, index = X.index, name = \"MEDV\")\n",
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2duyNVed6Bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObfbrMzYn3I",
        "colab_type": "text"
      },
      "source": [
        "## Filter approaches\n",
        "\n",
        "This kind of approach is based on thresholds, as only properties of the given characteristic are used to determine whether it should be selected or not. The most trivial example is discarding based on missing values, as we discussed before. This can be done directly with Pandas, using the `dropna()` method with the `thresh` parameter. Let's see if this would apply to our case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbXOXbI17G86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvD7S1jo7nBq",
        "colab_type": "text"
      },
      "source": [
        "Well, as with toy datasets, no missing values! Let's anyway see how this would work, if we had to do it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lATrPUt7JmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = X.shape[0]\n",
        "X.dropna(axis=1, thresh=0.1 * n_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yCA5eRo7yBn",
        "colab_type": "text"
      },
      "source": [
        "The code above removes all features for which over 90% of the values are missing. \n",
        "\n",
        "> Note that the approach above is not compatible with scikit-learn pipelines, so it should be applied even before data preparation ;)\n",
        "\n",
        "A second example of filter approach is based on variance, which filters out features for which the variance is below a given threshold. The variance filter is provided by scikit-learn as the `VarianceThreshold` resource. Let's apply this filter to our data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwKETsgwkNQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.var()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to03V3iRJ8WX",
        "colab_type": "text"
      },
      "source": [
        "> Hmm.. The variances are all over the place, so how can we set a common threshold? \n",
        "\n",
        "Remember that feature selection usually comes after data preparation, so at that point your numerical features should have unit standard deviation (which also means unit variance). Let's set the threshold to 0.1, to ensure only features with at least 10% variance will be preserved:\n",
        "\n",
        "> In the code below, we get the preserved column names using the `get_support()` method. This method provides a series of boolean values that we can use to identify which columns have been kept and which have been discarded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPbLy-JwkUGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "var_selector = VarianceThreshold(threshold=0.1)\n",
        "pipe = make_pipeline(MinMaxScaler(),\n",
        "                     StandardScaler(),\n",
        "                     var_selector)\n",
        "\n",
        "pipe.fit(X)\n",
        "column_mask = var_selector.get_support()\n",
        "pd.DataFrame(pipe.transform(X), \n",
        "             columns=X.columns[column_mask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVeTtRvkorFS",
        "colab_type": "text"
      },
      "source": [
        "Note that the no features have been discarded with a 0.1 threshold. Since we don't have many features, there's no need to increase this threshold here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACG3XWEaqItR",
        "colab_type": "text"
      },
      "source": [
        "## Wrapper approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0aqmc_Dqd5x",
        "colab_type": "text"
      },
      "source": [
        "This family of approaches builds models to understand feature importance. Overall, they can be categorized as **uni-** or **multivariate**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHFnFSyyqvE0",
        "colab_type": "text"
      },
      "source": [
        "### Univariate selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1fXItBqxZv",
        "colab_type": "text"
      },
      "source": [
        "Univariate selection is done using statistical tests on individual features. Test scores are used as measures of feature importance, allowing an absolute or relative number of features to be retrieved. Scikit-learn offers two resources for selection:\n",
        "\n",
        "- Absolute number of features: `SelectKBest`\n",
        "- Relative number of features: `SelectPercentile`\n",
        "\n",
        "Let's see a practical example using two different statistical tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEGEKf_7sLjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "kbest_selector = SelectKBest(f_regression, 5)\n",
        "pd.DataFrame(kbest_selector.fit_transform(X, y), \n",
        "             columns=X.columns[kbest_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x3vqrXmsp5q",
        "colab_type": "text"
      },
      "source": [
        "Let's discuss the example above for a minute. The statistical test applied is one of the options provided by scikit-learn for regression problems. It is based on the correlation between each characteristic and the target characteristic, so we have to provide `y` as an argument to `fit_transform`. Let's compare the selected features above with the correlation map between input and target features to see that they match:\n",
        "\n",
        "> If you missed the episode on cluster maps, check [pandas-zero](https://github.com/leobezerra/pandas-zero)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsX2aaeTvLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "full_data = pd.concat([X, y], axis=1)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.clustermap(full_data.corr(), annot=True, cmap=\"Reds\", \n",
        "               cbar_pos=None, dendrogram_ratio=0.1, fmt='.2g')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqG5NfNcwgjs",
        "colab_type": "text"
      },
      "source": [
        "Note that the five features are best correlate with the target feature are indeed `LSTAT` (-0.74), `RM`(0.7), `PTRATIO` (-0.51), `INDUS` (-0.48), and `TAX` (-0.47). We could have, instead, provided a statistical test based on mutual information between the input and target features. Let's see how that works, but selecting a relative number of features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUPvmkCVtRmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
        "percentile_selector = SelectPercentile(mutual_info_regression, 50)\n",
        "pd.DataFrame(percentile_selector.fit_transform(X, y), \n",
        "             columns=X.columns[percentile_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLvr4tpYtpku",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn offers several other statistics for regression and classification problems, but they would exceed the scope of this notebook. Let's move on to multivariate approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFDPPjRzuGrF",
        "colab_type": "text"
      },
      "source": [
        "### Multivariate selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWkMckCf3ISa",
        "colab_type": "text"
      },
      "source": [
        "Approaches that build models from multiple input features are called multivariate. Selection based on multivariate approaches can be done at a one-shot pass, or iteratively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKx7wRyC3WLS",
        "colab_type": "text"
      },
      "source": [
        "#### One-shot selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ufzoaFE3YqQ",
        "colab_type": "text"
      },
      "source": [
        "One-shot multivariate approaches fit a single model and select features based on the importance assigned by these models to each feature. Scikit-learn provides a handful of estimators that assign feature importance internally, and can then be used for this type of selection. Let's see a pratical example of that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxws355L3_GO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "oneshot_selector = SelectFromModel(RandomForestRegressor())\n",
        "pd.DataFrame(oneshot_selector.fit_transform(X, y), \n",
        "             columns=X.columns[oneshot_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDEBDDrr41zh",
        "colab_type": "text"
      },
      "source": [
        "> That was brutal!\n",
        "\n",
        "Let's understand what happened. First, we provided a random forest estimator to the `SelectFromModel` selector. Random forests are among the most used estimators for feature selection, as well as the following options:\n",
        "\n",
        "| Approach | Regression | Classification |\n",
        "| --- | --- | --- |\n",
        "| L1-norm | `Lasso`, `SVR` | `LogisticRegression` | \n",
        "| Tree-based | `ExtraTreesRegressor`  | `ExtraTreesClassifier`\n",
        "| | `RandomForestRegressor` | `RandomForestClassifier` | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5qBD_6I8Uuu",
        "colab_type": "text"
      },
      "source": [
        "Still, using a given estimator doesn't imply directly at such a brutal feature reduction. That happened because we didn't configure a threshold, so `SelectFromModel` used the mean feature importance as selection threshold. Let's see what happens if we used the median feature importance, instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "warKK_dV847p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oneshot_selector = SelectFromModel(RandomForestRegressor(),\n",
        "                                   threshold=\"median\")\n",
        "pd.DataFrame(oneshot_selector.fit_transform(X, y), \n",
        "             columns=X.columns[oneshot_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6NCdAR9CvB",
        "colab_type": "text"
      },
      "source": [
        "Note that now we get about half of the features, and that they are not necessarily a superset of the features we woulg get with univariate selection. It's also possible to set a maximum number of features to be returned, using parameter `max_features`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HELLIhLr9udo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oneshot_selector = SelectFromModel(RandomForestRegressor(),\n",
        "                                   threshold=\"median\",\n",
        "                                   max_features=5)\n",
        "pd.DataFrame(oneshot_selector.fit_transform(X, y), \n",
        "             columns=X.columns[oneshot_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJsuDFSu9tqu",
        "colab_type": "text"
      },
      "source": [
        "> Check the documentation on `SelectFromModel` to see how to configure selection based only on `max_features`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj1DEyb595u5",
        "colab_type": "text"
      },
      "source": [
        "#### Iterative selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-mWpic-BFx",
        "colab_type": "text"
      },
      "source": [
        "In contrast to one-shot selection, where feature importance is computed from a single model, iterative selection creates as many models as the number of features to be preserved/discarded. In scikit-learn, the `RFE` (recursive feature elimination) selector iteratively discard the least important feature: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFnn9iYB-0_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "iterative_selector = RFE(estimator=Lasso())\n",
        "pd.DataFrame(iterative_selector.fit_transform(X, y), \n",
        "             columns=X.columns[iterative_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q72N2keu_Wg8",
        "colab_type": "text"
      },
      "source": [
        "The `RFE` selector can be configured as to the total number of features to select and to the number of features to discard at each iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAYnq-7F_gHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterative_selector = RFE(estimator=Lasso(),\n",
        "                         n_features_to_select=5,\n",
        "                         step=2)\n",
        "pd.DataFrame(iterative_selector.fit_transform(X, y), \n",
        "             columns=X.columns[iterative_selector.get_support()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HnL43aqDSfm",
        "colab_type": "text"
      },
      "source": [
        "### Adding wrappers to the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyKqBi47EKbE",
        "colab_type": "text"
      },
      "source": [
        "As we have discussed before, everything that happens in machine learning is part of a pipeline. In scikit-learn, wrapper selectors should also be added to pipelines, as we did with data preparation. Let's see how that would work using two selectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_O7fz2EalV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prep_sel_pipe = make_pipeline(MinMaxScaler(),\n",
        "                              StandardScaler(),\n",
        "                              var_selector,\n",
        "                              iterative_selector\n",
        "                              )\n",
        "\n",
        "prep_sel_pipe.fit(X, y)\n",
        "column_mask = var_selector.get_support() & iterative_selector.get_support()\n",
        "pd.DataFrame(prep_sel_pipe.transform(X), columns=X.columns[column_mask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keGyUFzeG_-R",
        "colab_type": "text"
      },
      "source": [
        "Note that preserving feature names gets trickier with the addition of more components to the pipeline. To do that, we get the masks provided by each selector and combine them using the and operator `&`.\n",
        "\n",
        "> \n",
        "> Scikit-learn should make this easier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0QG0szO_tyU",
        "colab_type": "text"
      },
      "source": [
        "## Embedded selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaT_m8JlPF9c",
        "colab_type": "text"
      },
      "source": [
        "In filter and wrapper approaches, feature selection is a step that comes prior to estimation (even if wrapper approaches may use auxiliary model fitting). In embedded approaches, feature selection is done internally by the estimator during model fitting. Not all estimators provide this possibility, though. Besides the estimators discussed in the wrapper approach section, another estimator that deserves to be mentioned is the multi-layer perceptron.\n",
        "\n",
        "Note that the different selection approaches offer different trade-offs between the information level used for selection and computational cost required to produce that information level. In principle, embedded approaches should be preferred if the application domain is favorable to those estimators. Next, filter approaches are the computationally cheapest, but their selection capabilities are quite limited. Lastly, wrapper approaches are a very interesting alternative, with the particular methods for model fitting and selection being decided as a function of the problem one has at hand. An important thing to remember when using wrapper approaches, though, is to use cheap-to-compute auxiliary estimators, specially in iterative approaches."
      ]
    }
  ]
}