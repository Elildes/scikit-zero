{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Outliers and imputation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPB7Aq6r2_qJ",
        "colab_type": "text"
      },
      "source": [
        "# Outliers and imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eex39NvK9mku",
        "colab_type": "text"
      },
      "source": [
        "The first step in preparing the data for analysis concerns **outliers and missing values**. Outliers are data samples that are so different from the remaining that they can skew your analysis if not removed from the dataset. Missing values are values that for some reason have not been informed for given features of given samples. Aside from the missing information they would provide, scikit-learn estimators generally expect your `DataFrame` to be complete.\n",
        "\n",
        "To understand the resources provided by scikit-learn for this step, we'll use the New York City AirBnb dataset available at Kaggle. To download it, follow the [first stage of this tutorial](https://medium.com/@yvettewu.dw/tutorial-kaggle-api-google-colaboratory-1a054a382de0), which shows how to download access credentials for Kaggle (`kaggle.json`). Once you have downloaded the credentials, use the side menu to upload the file to Colab, and run the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZqaFTXVdCLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlDCAUMMdDXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!cp /content/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kidcUfidF-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download -d dgomonov/new-york-city-airbnb-open-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Pw1cCoAGEC",
        "colab_type": "text"
      },
      "source": [
        "> In this case, since the dataset is not the only file in the zip file, we first have to unzip everything before loading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3W8gO0UdfKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip new-york-city-airbnb-open-data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH6f5HHbACDq",
        "colab_type": "text"
      },
      "source": [
        "> Should any of the cells above fail, contact the maintainers of scikit-zero ;)\n",
        "\n",
        "Let's take a peek into the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFk2qKcrdQSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nyc_airbnb = pd.read_csv(\"AB_NYC_2019.csv\")\n",
        "nyc_airbnb.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ9mkASCh5sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nyc_airbnb.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oNB491iApod",
        "colab_type": "text"
      },
      "source": [
        "Note that now we have a much larger number of samples than in the iris dataset we used previously. Regarding the number of features, this dataset is actually small for real-world standards, but will help keep our notebook simple. Let's start dropping some features which won't help in our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4KWtP3DfZwB",
        "colab_type": "text"
      },
      "source": [
        "> `last_review` is a `datetime` field, which we could use for time series analysis. However, to make this notebook simple we're gonna discard it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGBL5GdeCSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discard = [\"id\", \"last_review\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QsQlWhGeOZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = nyc_airbnb.drop(discard, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKTRhw6HBHH7",
        "colab_type": "text"
      },
      "source": [
        "Before moving on to the specific topics of this analysis, let's review the basics on missing values. We can check them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bL4FsOrgPkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb9eCdtvBRGL",
        "colab_type": "text"
      },
      "source": [
        "In the case of features where very few samples have missing values, it's often safe to discard those samples. We can do that using the `dropna()` method, and specifying that we only want to drop samples for which the given subset of features present missing values (`subset=[\"name\", \"host_name\"]`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL40oLvUgT_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X.dropna(subset=[\"name\", \"host_name\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtdzhelzgaRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dsws1QRjgpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwhKbrCFCqep",
        "colab_type": "text"
      },
      "source": [
        "## Detecting outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0hMIBxRCt_E",
        "colab_type": "text"
      },
      "source": [
        "The very first step into data preparation is detecting and removing outliers from the data. To do that, we have to select which features we'll consider for this analysis, and in general they will be numerical. Looking at the feature distributions, we'll go with the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmqjBc26d49D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerical = [\"price\", \"minimum_nights\",\"number_of_reviews\", \n",
        "             \"calculated_host_listings_count\", \"availability_365\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et5hiIeoE5Mh",
        "colab_type": "text"
      },
      "source": [
        "Since these features all follow an exponential distribution, we'll start by transforming them using a logarithmic transformation:\n",
        "\n",
        "> Check [pandas-zero](https://github.com/leobezerra/pandas-zero) if you missed that episode ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAcZrDfL9FfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntCI7Zjg9HJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.loc[:,numerical] = X[numerical].apply(np.log1p, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UciZY-XnuB60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(X[\"price\"], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFUZewU6FMvZ",
        "colab_type": "text"
      },
      "source": [
        "Note that now the numerical features we selected follow a distribution that is more similar to a normal distribution. The next step is selecting an unsupervised learning algorithm to cluster the data and identify the samples that do not belong to that big cluster. Scikit-learn offers a few options, and here we're gonna take **local outlier factor** (LOF) as an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjl6v5jgdcHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "clf = LocalOutlierFactor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL7vcEXeGGbw",
        "colab_type": "text"
      },
      "source": [
        "Let's isolate the numerical features to easen our task:\n",
        "\n",
        "> Since the index of the data is preserved, we'll be able to apply the insights obtained from X_num to X later ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRzzS2SkFnwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_num = X[numerical]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFQekRi_Geo4",
        "colab_type": "text"
      },
      "source": [
        "In the context of outlier detection, we use the `fit_predict()` method directly on the input features, without having a target feature `y` to predict. The predicted values will either be 1 (an inlier) or -1 (an outlier), according to the internal model produced by LOF. Since the output is provided as a numpy array, we wrap it as a Pandas `Series` and specify that we want to preserve the index from `X_num`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcg5D4kidvoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = pd.Series(clf.fit_predict(X_num), name=\"predicted\", index=X_num.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVyh7HpQHPoJ",
        "colab_type": "text"
      },
      "source": [
        "Now we can concatenate the `X_num` dataframe with the predicted labels, and query those who were identified as outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVoKXoOphctU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_predicted = pd.concat([X_num, predicted], axis=1)\n",
        "X_predicted.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTKpmBHsiV16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_outliers = X_predicted.query(\"predicted == -1\")\n",
        "X_outliers.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJvOo92LuwYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_outliers.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvhprDZcH1yN",
        "colab_type": "text"
      },
      "source": [
        "Since we have preserved the index, we can use them to drop the samples in `X` that LOF indicated as outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q_4fG1buyw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X.drop(X_predicted.query(\"predicted == -1\").index)\n",
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i1OayOyu7qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb_-CE5TIBVu",
        "colab_type": "text"
      },
      "source": [
        "## Imputing missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF8Zu_eD2s9W",
        "colab_type": "text"
      },
      "source": [
        "Missing values can compromise the ability of an estimator, particularly the ones in scikit-learn. As we've done previously, sometimes it makes sense to drop samples or even whole features when the proportion of missing values allows that:\n",
        "\n",
        "* if only a few samples present missing values for a given feature, the samples could likely be discarded.\n",
        "* when almost all samples present missing values for a given feature, the feature could likely be discarded.\n",
        "\n",
        "When none of the conditions above apply, the typical approach is to **impute** data, i.e., to produce artificial values based on the available data. The types of imputation methods vary as to the nature of the data used, as we discuss next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2XthTrQIdrl",
        "colab_type": "text"
      },
      "source": [
        "### Based solely on the given feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKZxD4KM5KjU",
        "colab_type": "text"
      },
      "source": [
        "The simplest imputation approach is to fill values of a given feature based only on the values available for that feature from the remaining samples of the dataset. This is provided by scikit-learn as the `SimpleImputer` preprocessor, which fills missing values with the feature mean, by default:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_gJVTXbIDrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "mean_imputer = SimpleImputer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEVgIVpw5irO",
        "colab_type": "text"
      },
      "source": [
        "Since we're only interested in imputing the `reviews_per_month` feature, we'll use the `make_column_transformer` method from the `compose` module. This method allows us to specify preprocessing approaches for specific features.\n",
        "\n",
        "- Let's understand the code below:\n",
        "```python \n",
        "mean_transformer = make_column_transformer(\n",
        "                                             (mean_imputer, [\"reviews_per_month\"]),\n",
        "                                             remainder=\"drop\"\n",
        "                                            )\n",
        "```\n",
        "- Creates the column transformer, specifying that `mean_imputer` should be applied to `[\"reviews_per_month\"]`, and that the remainder of the features should be dropped. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGJGV1CgQEVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "mean_transformer = make_column_transformer(\n",
        "                                           (mean_imputer, [\"reviews_per_month\"]),\n",
        "                                           remainder=\"drop\"\n",
        "                                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1706lE26_oW",
        "colab_type": "text"
      },
      "source": [
        "Once we have the transformer ready, we can use its `fit_transform()` method to impute the data, wrapping it in a Pandas `DataFrame` where we preserve the index from the original data for when we want to replace the original missing values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXzyFY6JSEYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_mean = pd.DataFrame(mean_transformer.fit_transform(X), columns=[\"reviews_per_month\"], index=X.index)\n",
        "X_mean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0r0rKoH7nww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_mean.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEfBqz117lS6",
        "colab_type": "text"
      },
      "source": [
        "As we can see, every missing value has been replaced. Let's compare this distribution of this feature before and after imputation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSdD49n5IyDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(X[\"reviews_per_month\"], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB05efjmJg_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(X_mean[\"reviews_per_month\"], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBsMn5G73-x",
        "colab_type": "text"
      },
      "source": [
        "The different approaches for `SimpleImputer` are meant to help the user find a strategy that impact less the data distribution. For instance, we could have tried replacing the missing values with the mode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgB33zgJn6x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "mode_transformer = make_column_transformer(\n",
        "                                           (mode_imputer, [\"reviews_per_month\"]),\n",
        "                                           remainder=\"drop\"\n",
        "                                           )\n",
        "X_mode = pd.DataFrame(mode_transformer.fit_transform(X), columns=[\"reviews_per_month\"], index=X.index)\n",
        "sns.distplot(X_mode[\"reviews_per_month\"], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAY6C238QF_",
        "colab_type": "text"
      },
      "source": [
        "Notice that, in this case, the imputed feature distribution is very similar whether the mean or mode has been used, but that may not always be the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn14nyLpLg-w",
        "colab_type": "text"
      },
      "source": [
        "### Based on multiple features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XNOgLK_8ZQY",
        "colab_type": "text"
      },
      "source": [
        "A more robust approach to data imputation is determining artificial values based on multiple features (preferably, the whole dataset). In the example below, we're gonna use the numerical features to aid us impute the missing values for `reviews_per_month`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ul7phsgLzqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "knn_imputer = KNNImputer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYuucqys9DFb",
        "colab_type": "text"
      },
      "source": [
        "Let's isolate the numerical features once again, this time including the feature we want to impute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rUrYrUjOE45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerical = [\"price\", \"minimum_nights\",\"number_of_reviews\", \"reviews_per_month\",\n",
        "             \"calculated_host_listings_count\", \"availability_365\"]\n",
        "X_num = X[numerical]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nvHrRZY9TqN",
        "colab_type": "text"
      },
      "source": [
        "Imputing is performed the same way using `fit_transform()`, wrapping the result in a `DataFrame`and preserving column and index information:\n",
        "\n",
        "> Note that the cell below takes a little longer to run, because a kNN model is fit internally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNkC1Q2CNSlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_imputed = pd.DataFrame(knn_imputer.fit_transform(X_num), columns=X_num.columns, index=X_num.index)\n",
        "X_imputed.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st360LwW9lkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_imputed.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26GnU8g29kds",
        "colab_type": "text"
      },
      "source": [
        "Let's compare the data distribution before and after imputation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFSRKjGz9yyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(X[\"reviews_per_month\"], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvAEQPIeNsAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(X_imputed[\"reviews_per_month\"], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aDs2ke8934f",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the imputed version is now much more similar to the original data distribution. "
      ]
    }
  ]
}