{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First steps",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwJLE2VliJCg",
        "colab_type": "text"
      },
      "source": [
        "# First steps with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHqm8e69iNXx",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn is by far the most used machine learning library in the Python community. It is so comprehensive that we are gonna limit ourselves to a supervised learning, classification examples to illustrate the most important ideas behind this library.\n",
        "\n",
        "> This notebook is a critical review of the [Getting started](https://scikit-learn.org/stable/getting_started.html) entry from the original scikit-learn documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCWYq-9GjDGv",
        "colab_type": "text"
      },
      "source": [
        "As the official documentation states, scikit-learn is built mostly on SciPy, NumPy, and Matplotlib. If you've been around since pandas-zero, you know that we can use more high-level libraries that help us do the same work.\n",
        "\n",
        "> This notebook assumes you've already took the [pandas-zero](https://github.com/leobezerra/pandas-zero) course. If you haven't, please do 😉"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVd3Lw9Po_8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a_mO4IZpxKI",
        "colab_type": "text"
      },
      "source": [
        "## A way too simplistic example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRkqpCbGpze3",
        "colab_type": "text"
      },
      "source": [
        "The first goal of this tutorial is to have you understand the basic objects and conventions in scikit-learn. For this reason, let's start with a way too simplistic example for real life standards.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwtHb4RDi-17",
        "colab_type": "text"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6BiiTEsj3wr",
        "colab_type": "text"
      },
      "source": [
        "We'll load the iris dataset from Seaborn, which is represented as a `DataFrame` from Pandas. \n",
        "\n",
        "> This dataset is way too simplistic for real life standards, but I told you it would be 🙃"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5SjR3hTyjiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_dataset = sns.load_dataset('iris')\n",
        "iris_dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wN9oSN9kyy2",
        "colab_type": "text"
      },
      "source": [
        "As we can see above, this dataset presents four numerical features and a target feature that represents the flower species to which the sample belongs. By convention, we isolate the target feature from the rest of the features:\n",
        "\n",
        "> `X` stands for features that will be used for prediction.\n",
        "\n",
        "> `y` stands for the target feature, or **label**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0eJnDhsnAvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris_dataset.iloc[:,0:4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPvSXQzhnD3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = iris_dataset[\"species\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMgIIlFEkSqn",
        "colab_type": "text"
      },
      "source": [
        "### Selecting a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs85I6SzkWCD",
        "colab_type": "text"
      },
      "source": [
        "In a supervised learning machine learning problem, we have to select a classification algorithm (or **classifier**) to learn the patterns that define each class. scikit-learn offers a wide range of classifiers, so we'll stick to the simplest one: kNN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI4OU-R9llDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTXof2LYluPi",
        "colab_type": "text"
      },
      "source": [
        "kNN is short for k-nearest neighbors. In a nutshell, kNN predicts the label for a given sample based on the labels from its k nearest neighbors. \n",
        "\n",
        "> By default, kNN considers nearest neighbors according to Euclidean distance. \n",
        "\n",
        "Creating a classifier is very straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzBc1oGBzGA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = KNeighborsClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WQ4TGT1mme2",
        "colab_type": "text"
      },
      "source": [
        "### Fitting a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i8W5XWomqW-",
        "colab_type": "text"
      },
      "source": [
        "Every classifier in scikit-learn provides a method `fit()`, which builds a model for estimating labels `y` from features `X`. Once again, fitting a model is pretty straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A52z0rZCzK77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY9KklZam5-C",
        "colab_type": "text"
      },
      "source": [
        "Note that the output of the cell above is very verbose. It represents all the inner aspects of the classifier that scikit-learn allows you to customize when creating one.\n",
        "\n",
        "> We'll talk about this later, since it deserves a whole notebook!\n",
        "\n",
        "Also, note that this output is just a log that scikit-learn produces. The model fitting is done internally by the `clf` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZFh2P8WoKun",
        "colab_type": "text"
      },
      "source": [
        "### Predicting labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1g8VLqToOAB",
        "colab_type": "text"
      },
      "source": [
        "Once we have fitted a model to the data, we can use it to predict labels. scikit-learn classifiers provide a method `predict()`, which predicts labels for given samples:\n",
        "\n",
        "> By convention, we refer to labels predicted by classifiers as `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR24GTtLzie-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIrZqjDworOp",
        "colab_type": "text"
      },
      "source": [
        "As discussed in the beginning, scikit-learn is built on NumPy, so the output of the method is an `ndarray` from NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GilfEjyconY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leeKa9f3o1iz",
        "colab_type": "text"
      },
      "source": [
        "Since that is not very high-level, let's convert it to a Pandas `Series`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At2jhabso7mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = pd.Series(y_pred)\n",
        "y_pred.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuaEN5ejpJhX",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N-auC90pRas",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn offers a number of approaches to evaluate the quality of a prediction. The simplest one is called a confusion matrix: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM573_krzr4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIWbYArzsyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(y, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdbfmrPUqpS3",
        "colab_type": "text"
      },
      "source": [
        "Not very readable, is it? Let's plot this and I'll walk you through it.\n",
        "\n",
        "> Note that the `plot_confusion_matrix()` method below takes as arguments the fitted classifier, the input features and the target labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIKvltqnl32X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dunQEqoKmp0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(clf, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nS_C-wIx43Z",
        "colab_type": "text"
      },
      "source": [
        "Now we can see things more clearly. On the rows, we have the predicted labels. On the columns, the true labels. Ideally, all samples should be accounted on the main diagonal, which would mean the were predicted correctly.\n",
        "\n",
        "That's what happens to the setosa species. kNN is able to create a model that correctly classifies all of its examples. For the remaining classes, kNN mistakes three versicolor samples for virginica, and two virginica samples for versicolor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FX324gJyaUn",
        "colab_type": "text"
      },
      "source": [
        "## A still simplistic example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB-qCEqWzipE",
        "colab_type": "text"
      },
      "source": [
        "In the example above, a very simplistic assumption is that the classifier could see all the samples to fit its model. In practice, a model fitted like that would probably **overfit** to the data it has seen. In real life, we use **sampling** to promote **generalization**.\n",
        "\n",
        "> Sampling means that the classifier will be evaluated on data that it has not seen when fitting its model.\n",
        "\n",
        "> Generalization is the ability of a classifier to predict labels correctly for samples it has not seen when fitting the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuf3U9Ql0TQd",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn offers several sampling approaches. The simplest is called **holdout**, and is provided as the `train_test_split()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqcGJVsFnpzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mCXIry5nzKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOpjUtRy0c6_",
        "colab_type": "text"
      },
      "source": [
        "Note that the original data `X` and `y` is now partitioned into two subsets:\n",
        "\n",
        "> `X_train` and `y_train` will be used to fit the model. They are know as training data.\n",
        "\n",
        "> `X_test` and `y_test` will be used to evaluate the model. They are know as test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWDqiygHoGRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkMCKYxcn-71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(clf, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj9nS9uA1Jlz",
        "colab_type": "text"
      },
      "source": [
        "Note that the number of samples used for testing is much smaller than the number of samples used for training. This ratio is a parameter of `train_test_split`, which one can configure when splitting the data.\n",
        "\n",
        "> Once again, sampling is so important that it will have its own notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esKHIkd70yXg",
        "colab_type": "text"
      },
      "source": [
        "## A less simplistic example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctyWf-Ga1uaW",
        "colab_type": "text"
      },
      "source": [
        "Another very simplistic assumption we made in the examples above is that the data is ready for modelling as it comes. \n",
        "\n",
        "> Since we're using the iris dataset, this is still kinda true.\n",
        "\n",
        "In real life, however, fitting a model is the last stage in a machine learning **pipeline**. \n",
        "\n",
        "> And if you have taken the pandas-zero course, machine learning is a very late stage in the data science process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgbSCRS625OX",
        "colab_type": "text"
      },
      "source": [
        "### Understanding pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTAUrP5q28Xt",
        "colab_type": "text"
      },
      "source": [
        "Pipelines are the atomic unit in machine learning. They comprise:\n",
        "* Data preparation\n",
        "* Feature engineering\n",
        "* Estimators\n",
        "\n",
        "> All these topics are super important, so each will have their own notebook 😁\n",
        "\n",
        "The classifier we have been using plays the role of the estimator. Since this is still a simplistic example, I'll show you how to build a very simple pipeline where we include a data preparation component."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_CuCeEe3lIa",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8_gXl974BlH",
        "colab_type": "text"
      },
      "source": [
        "Even if the iris dataset is quite simplistic, its features present very different ranges:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXlcK3v-hk1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_dataset.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mScXuaGO4MHG",
        "colab_type": "text"
      },
      "source": [
        "In general, machine learning algorithms should deal with features that have been rescaled to present a similar range. In scikit-learn, we can do this using the `MinMaxScaler` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXkqiDawof7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmDtP6Zo4nPt",
        "colab_type": "text"
      },
      "source": [
        "Next, we create our pipeline using the `make_pipeline()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-syrt2KAotCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(scaler, clf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvK0akDg53l8",
        "colab_type": "text"
      },
      "source": [
        "A pipeline object follows the same fit-predict pattern from classifiers:\n",
        "\n",
        "> Note that the the model that had been fitted by `clf` will be lost, since the pipeline will refit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-MhPvuo5dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOgbpuNv6BQU",
        "colab_type": "text"
      },
      "source": [
        "This output is even more verbose and starts to give you an idea on how complex creating and configuring a machine learning pipeline may become. Let's see if adding the data preparation component affected results somehow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQVBwN92o8KP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(pipe, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DBBYTN_66Es",
        "colab_type": "text"
      },
      "source": [
        "The output you see above may be better, equal, or worse than the result presented in the previous section. The reason is that the `train_test_split()` method splits the data in a randomized way, so it's not really possible to reproduce the exact results everytime.\n",
        "\n",
        "> This is possible in a controlled setup, and we'll discuss it in a notebook just for that 🤓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B05Xh0PCD53u",
        "colab_type": "text"
      },
      "source": [
        "More importantly, this means that every component in a pipeline might bring benefits or worsen results, which makes creating and configuring pipelines quite challenging and dataset-specific."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjDWiYeREJE6",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating models analytically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsFKMGfQEMcQ",
        "colab_type": "text"
      },
      "source": [
        "To conclude our notebook, let's switch from a graphical analysis to an analytical one. This is very important in real life, since it is not feasible to use confusion matrices for many datasets.\n",
        "\n",
        "> Guess what? Yep. A notebook for that 😎"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-jS0-jjE15N",
        "colab_type": "text"
      },
      "source": [
        "For classification problems where the number of samples is evenly distributed among the different classes, the most used metric is called accuracy.\n",
        "\n",
        "> Accuracy is the ration between the number of samples correctly classified and the total number of samples.\n",
        "\n",
        "We can use the `accuracy_score()` method from scikit-learn to compute it: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNqXuoFKpL9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_UnGpk0pGN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1YmpmgpX-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = pipe.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXrT0uJPF4qa",
        "colab_type": "text"
      },
      "source": [
        "Once again, whether results are better, equal, or worse depend on the run. Yet, we now have an analytical metric that assertains that higher scores represent better prediction."
      ]
    }
  ]
}